{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e5388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# For progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# For splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696aa9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('../Data/train_augmented.csv')\n",
    "test=pd.read_csv('../Data/test_df.csv')\n",
    "val=pd.read_csv('../Data/val_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b75770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotions</th>\n",
       "      <th>path</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sad</td>\n",
       "      <td>augmented_audio/03-01-04-02-02-02-08_aug_0.wav</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>calm</td>\n",
       "      <td>augmented_audio/03-02-02-01-01-02-12_aug_1.wav</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>augmented_audio/03-01-05-01-01-01-09_aug_1.wav</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>calm</td>\n",
       "      <td>augmented_audio/03-02-02-02-01-02-01_aug_1.wav</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sad</td>\n",
       "      <td>augmented_audio/03-02-04-01-01-01-22_aug_1.wav</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4405</th>\n",
       "      <td>calm</td>\n",
       "      <td>augmented_audio/03-02-02-01-02-02-21_aug_0.wav</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4406</th>\n",
       "      <td>calm</td>\n",
       "      <td>augmented_audio/03-01-02-02-02-02-02_aug_0.wav</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4407</th>\n",
       "      <td>neutral</td>\n",
       "      <td>augmented_audio/03-02-01-01-01-02-22_aug_1.wav</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4408</th>\n",
       "      <td>surprise</td>\n",
       "      <td>augmented_audio/03-01-08-02-02-02-21_aug_0.wav</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409</th>\n",
       "      <td>calm</td>\n",
       "      <td>augmented_audio/03-01-02-01-01-02-05_aug_1.wav</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4410 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotions                                            path type\n",
       "0          sad  augmented_audio/03-01-04-02-02-02-08_aug_0.wav  NaN\n",
       "1         calm  augmented_audio/03-02-02-01-01-02-12_aug_1.wav  NaN\n",
       "2        angry  augmented_audio/03-01-05-01-01-01-09_aug_1.wav  NaN\n",
       "3         calm  augmented_audio/03-02-02-02-01-02-01_aug_1.wav  NaN\n",
       "4          sad  augmented_audio/03-02-04-01-01-01-22_aug_1.wav  NaN\n",
       "...        ...                                             ...  ...\n",
       "4405      calm  augmented_audio/03-02-02-01-02-02-21_aug_0.wav  NaN\n",
       "4406      calm  augmented_audio/03-01-02-02-02-02-02_aug_0.wav  NaN\n",
       "4407   neutral  augmented_audio/03-02-01-01-01-02-22_aug_1.wav  NaN\n",
       "4408  surprise  augmented_audio/03-01-08-02-02-02-21_aug_0.wav  NaN\n",
       "4409      calm  augmented_audio/03-01-02-01-01-02-05_aug_1.wav  NaN\n",
       "\n",
       "[4410 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e4d1c",
   "metadata": {},
   "source": [
    "# Features in Audio Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b845f0d",
   "metadata": {},
   "source": [
    "## MFCC\n",
    "- What they are: The absolute gold standard for audio classification. They represent the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.\n",
    "- Why they're great: They are excellent at capturing the timbre and textural qualities of a sound, which are highly correlated with emotion in speech (e.g., the \"raspiness\" in an angry voice vs. the \"smoothness\" of a sad voice). They are also designed to mimic human hearing perception.\n",
    "- How many: Typically, you'll extract between 13 and 40 MFCCs per frame. A good starting point is 20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5e804",
   "metadata": {},
   "source": [
    "## Chroma Features (Chromagram)\n",
    "- What they are: A representation of the 12 distinct pitch classes (C, C#, D, etc.) of the musical octave.\n",
    "- Why they're useful: While primarily for music, they can capture intonation and pitch contours in speech. A rising intonation might signal a question or surprise, while a falling one might indicate a statement or sadness. This is particularly useful for the RAVDESS dataset since it contains both speech and song."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f868f49",
   "metadata": {},
   "source": [
    "## Mel Spectrogram\n",
    "- What it is: A spectrogram where the frequencies are converted to the mel scale. It's essentially the step right before calculating MFCCs.\n",
    "- Why it's useful: It's a rich representation of the sound. Some advanced models, especially CNNs, can work directly on Mel Spectrograms as if they were images, learning the features automatically. It's a great alternative or addition to MFCCs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92076fb",
   "metadata": {},
   "source": [
    "## Other Useful Features (Per-Frame)\n",
    "- Spectral Centroid: Indicates the \"center of mass\" of the spectrum. It relates to the \"brightness\" of a sound. An angry or surprised voice is often \"brighter.\"\n",
    "- Zero-Crossing Rate: The rate at which the signal changes sign (from positive to negative). It can help distinguish between voiced speech (low ZCR) and unvoiced/noisy sounds like 'sh' or static (high ZCR).\n",
    "- RMS (Root Mean Square) Energy: Corresponds to the loudness or amplitude of the audio frame. Anger is often louder, while sadness can be quieter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec991f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MFCC = 20\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "def extract_features_sequential(file_path):\n",
    "    \"\"\"Extracts MFCCs from an audio file and returns as a sequence.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        # Transpose to get (time_steps, n_features)\n",
    "        return mfccs.T\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "842cedb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features... (This may take a while)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98e6fdbe8674442b470a271f4443d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f36340b0ddb41ffa93bd6467c8127c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d5ad1476ac4b5689b9135835467f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- Step 3: Apply Feature Extraction to all DataFrames ---\n",
    "print(\"\\nExtracting features... (This may take a while)\")\n",
    "train['features'] = train['path'].progress_apply(extract_features_sequential)\n",
    "val['features'] = val['path'].progress_apply(extract_features_sequential)\n",
    "test['features'] = test['path'].progress_apply(extract_features_sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d362a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using fixed sequence length: 223\n",
      "\n",
      "Shape of padded training features: (4410, 223, 20)\n",
      "Shape of padded validation features: (491, 223, 20)\n",
      "Shape of padded test features: (491, 223, 20)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where feature extraction failed\n",
    "train.dropna(subset=['features'], inplace=True)\n",
    "val.dropna(subset=['features'], inplace=True)\n",
    "test.dropna(subset=['features'], inplace=True)\n",
    "\n",
    "# Determine a fixed length for sequences (e.g., 95th percentile of lengths)\n",
    "all_lengths = pd.concat([train['features'], val['features'], test['features']]).apply(len)\n",
    "FIXED_LENGTH = int(all_lengths.quantile(0.95))\n",
    "print(f\"\\nUsing fixed sequence length: {FIXED_LENGTH}\")\n",
    "\n",
    "# Pad or truncate the features\n",
    "X_train = pad_sequences(train['features'].tolist(), maxlen=FIXED_LENGTH, padding='post', truncating='post', dtype='float32')\n",
    "X_val = pad_sequences(val['features'].tolist(), maxlen=FIXED_LENGTH, padding='post', truncating='post', dtype='float32')\n",
    "X_test = pad_sequences(test['features'].tolist(), maxlen=FIXED_LENGTH, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "print(f\"\\nShape of padded training features: {X_train.shape}\")\n",
    "print(f\"Shape of padded validation features: {X_val.shape}\")\n",
    "print(f\"Shape of padded test features: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80db41d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of one-hot encoded training labels: (4410, 8)\n",
      "Label classes: ['angry' 'calm' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Encode the Target Labels ---\n",
    "\n",
    "# Get the corresponding labels\n",
    "y_train = train['emotions']\n",
    "y_val = val['emotions']\n",
    "y_test = test['emotions']\n",
    "\n",
    "# Use LabelEncoder to convert emotion strings to integers\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_val_encoded = le.transform(y_val)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# One-hot encode the integer labels for the model's output layer\n",
    "y_train_categorical = to_categorical(y_train_encoded)\n",
    "y_val_categorical = to_categorical(y_val_encoded)\n",
    "y_test_categorical = to_categorical(y_test_encoded)\n",
    "\n",
    "print(f\"\\nShape of one-hot encoded training labels: {y_train_categorical.shape}\")\n",
    "# Save the label encoder classes for later prediction decoding\n",
    "print(f\"Label classes: {le.classes_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd4ed155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting scaler on training data...\n",
      "\n",
      "Shape of final scaled training features: (4410, 223, 20)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Scale the Features ---\n",
    "# This is a crucial step for model performance\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape data from 3D to 2D for the scaler\n",
    "nsamples_train, nsteps_train, nfeatures_train = X_train.shape\n",
    "X_train_2d = X_train.reshape((nsamples_train * nsteps_train, nfeatures_train))\n",
    "\n",
    "# Fit the scaler ONLY on the training data\n",
    "print(\"\\nFitting scaler on training data...\")\n",
    "scaler.fit(X_train_2d)\n",
    "\n",
    "# Transform the training, validation, and test data\n",
    "X_train_scaled_2d = scaler.transform(X_train_2d)\n",
    "\n",
    "# Reshape validation and test sets to 2D\n",
    "nsamples_val, nsteps_val, nfeatures_val = X_val.shape\n",
    "X_val_2d = X_val.reshape((nsamples_val * nsteps_val, nfeatures_val))\n",
    "X_val_scaled_2d = scaler.transform(X_val_2d)\n",
    "\n",
    "nsamples_test, nsteps_test, nfeatures_test = X_test.shape\n",
    "X_test_2d = X_test.reshape((nsamples_test * nsteps_test, nfeatures_test))\n",
    "X_test_scaled_2d = scaler.transform(X_test_2d)\n",
    "\n",
    "# Reshape all data back to 3D for the sequence model\n",
    "X_train_scaled = X_train_scaled_2d.reshape(X_train.shape)\n",
    "X_val_scaled = X_val_scaled_2d.reshape(X_val.shape)\n",
    "X_test_scaled = X_test_scaled_2d.reshape(X_test.shape)\n",
    "\n",
    "print(f\"\\nShape of final scaled training features: {X_train_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd5d3f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preprocessing Complete ---\n",
      "You now have the following model-ready variables:\n",
      "X_train_scaled: (4410, 223, 20)\n",
      "y_train_categorical: (4410, 8)\n",
      "X_val_scaled: (491, 223, 20)\n",
      "y_val_categorical: (491, 8)\n",
      "X_test_scaled: (491, 223, 20)\n",
      "y_test_categorical: (491, 8)\n",
      "\n",
      "These are ready to be fed into a sequence model like an LSTM or GRU.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Preprocessing Complete ---\")\n",
    "print(\"You now have the following model-ready variables:\")\n",
    "print(f\"X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"y_train_categorical: {y_train_categorical.shape}\")\n",
    "print(f\"X_val_scaled: {X_val_scaled.shape}\")\n",
    "print(f\"y_val_categorical: {y_val_categorical.shape}\")\n",
    "print(f\"X_test_scaled: {X_test_scaled.shape}\")\n",
    "print(f\"y_test_categorical: {y_test_categorical.shape}\")\n",
    "print(\"\\nThese are ready to be fed into a sequence model like an LSTM or GRU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643bcae",
   "metadata": {},
   "source": [
    "# Saving the files for Later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0382c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'processed_data'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc98df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Save the NumPy arrays ---\n",
    "# We use np.savez_compressed to save multiple arrays into a single,\n",
    "# efficient .npz file. This is much better than saving 6 separate files.\n",
    "\n",
    "data_arrays_path = os.path.join(output_dir, 'audio_data_processed.npz')\n",
    "np.savez_compressed(\n",
    "    data_arrays_path,\n",
    "    X_train=X_train_scaled,\n",
    "    X_val=X_val_scaled,\n",
    "    X_test=X_test_scaled,\n",
    "    y_train=y_train_categorical,\n",
    "    y_val=y_val_categorical,\n",
    "    y_test=y_test_categorical\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24e14cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data saving complete! ---\n",
      "Arrays saved to: processed_data/audio_data_processed.npz\n",
      "Scaler saved to: processed_data/scaler.joblib\n",
      "Label encoder saved to: processed_data/label_encoder.joblib\n",
      "\n",
      "Find these files in the 'processed_data' directory.\n"
     ]
    }
   ],
   "source": [
    "scaler_path = os.path.join(output_dir, 'scaler.joblib')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "label_encoder_path = os.path.join(output_dir, 'label_encoder.joblib')\n",
    "joblib.dump(le, label_encoder_path)\n",
    "\n",
    "print(f\"--- Data saving complete! ---\")\n",
    "print(f\"Arrays saved to: {data_arrays_path}\")\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "print(f\"Label encoder saved to: {label_encoder_path}\")\n",
    "print(f\"\\nFind these files in the '{output_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc00d5d",
   "metadata": {},
   "source": [
    "# Feature Engineering After Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9d2e6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotions</th>\n",
       "      <th>path</th>\n",
       "      <th>type</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sad</td>\n",
       "      <td>augmented_audio/03-01-04-02-02-02-08_aug_0.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[-483.3294, 0.91852564, 8.517652, 5.6402187, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>calm</td>\n",
       "      <td>augmented_audio/03-02-02-01-01-02-12_aug_1.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[-770.82745, 2.6873112, 2.682191, 2.6736703, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>angry</td>\n",
       "      <td>augmented_audio/03-01-05-01-01-01-09_aug_1.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[-337.3017, -9.244927, -14.508394, 9.422054, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>calm</td>\n",
       "      <td>augmented_audio/03-02-02-02-01-02-01_aug_1.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[-260.23245, 3.3638034, 6.661133, 0.99075234,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sad</td>\n",
       "      <td>augmented_audio/03-02-04-01-01-01-22_aug_1.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[-751.0063, 1.6978652, 1.6965203, 1.6942803, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4405</th>\n",
       "      <td>calm</td>\n",
       "      <td>augmented_audio/03-02-02-01-02-02-21_aug_0.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[-275.2522, 32.63665, -30.75, 38.10554, -27.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4406</th>\n",
       "      <td>calm</td>\n",
       "      <td>augmented_audio/03-01-02-02-02-02-02_aug_0.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[-847.4977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4407</th>\n",
       "      <td>neutral</td>\n",
       "      <td>augmented_audio/03-02-01-01-01-02-22_aug_1.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[-395.50806, -2.6607165, -1.1190903, 1.626747...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4408</th>\n",
       "      <td>surprise</td>\n",
       "      <td>augmented_audio/03-01-08-02-02-02-21_aug_0.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[-701.22375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409</th>\n",
       "      <td>calm</td>\n",
       "      <td>augmented_audio/03-01-02-01-01-02-05_aug_1.wav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[-320.13116, -3.3925047, 5.168479, -1.0326539...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4410 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      emotions                                            path type  \\\n",
       "0          sad  augmented_audio/03-01-04-02-02-02-08_aug_0.wav  NaN   \n",
       "1         calm  augmented_audio/03-02-02-01-01-02-12_aug_1.wav  NaN   \n",
       "2        angry  augmented_audio/03-01-05-01-01-01-09_aug_1.wav  NaN   \n",
       "3         calm  augmented_audio/03-02-02-02-01-02-01_aug_1.wav  NaN   \n",
       "4          sad  augmented_audio/03-02-04-01-01-01-22_aug_1.wav  NaN   \n",
       "...        ...                                             ...  ...   \n",
       "4405      calm  augmented_audio/03-02-02-01-02-02-21_aug_0.wav  NaN   \n",
       "4406      calm  augmented_audio/03-01-02-02-02-02-02_aug_0.wav  NaN   \n",
       "4407   neutral  augmented_audio/03-02-01-01-01-02-22_aug_1.wav  NaN   \n",
       "4408  surprise  augmented_audio/03-01-08-02-02-02-21_aug_0.wav  NaN   \n",
       "4409      calm  augmented_audio/03-01-02-01-01-02-05_aug_1.wav  NaN   \n",
       "\n",
       "                                               features  \n",
       "0     [[-483.3294, 0.91852564, 8.517652, 5.6402187, ...  \n",
       "1     [[-770.82745, 2.6873112, 2.682191, 2.6736703, ...  \n",
       "2     [[-337.3017, -9.244927, -14.508394, 9.422054, ...  \n",
       "3     [[-260.23245, 3.3638034, 6.661133, 0.99075234,...  \n",
       "4     [[-751.0063, 1.6978652, 1.6965203, 1.6942803, ...  \n",
       "...                                                 ...  \n",
       "4405  [[-275.2522, 32.63665, -30.75, 38.10554, -27.9...  \n",
       "4406  [[-847.4977, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...  \n",
       "4407  [[-395.50806, -2.6607165, -1.1190903, 1.626747...  \n",
       "4408  [[-701.22375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....  \n",
       "4409  [[-320.13116, -3.3925047, 5.168479, -1.0326539...  \n",
       "\n",
       "[4410 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c92ca6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "SAMPLE_RATE = 22050\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "N_MFCC = 13\n",
    "\n",
    "# === ADVANCED FEATURE EXTRACTION ===\n",
    "def extract_advanced_features(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "\n",
    "        # Static Features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "        rms = librosa.feature.rms(y=y, frame_length=N_FFT, hop_length=HOP_LENGTH)\n",
    "\n",
    "        # Dynamic Features\n",
    "        delta_mfccs = librosa.feature.delta(mfccs)\n",
    "        delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "\n",
    "        # Synchronize feature lengths\n",
    "        ref_len = mfccs.shape[1]\n",
    "        def sync(feat):\n",
    "            if feat.shape[1] > ref_len:\n",
    "                return feat[:, :ref_len]\n",
    "            elif feat.shape[1] < ref_len:\n",
    "                return np.pad(feat, ((0, 0), (0, ref_len - feat.shape[1])), mode='constant')\n",
    "            return feat\n",
    "\n",
    "        features = np.vstack([\n",
    "            mfccs,\n",
    "            delta_mfccs,\n",
    "            delta2_mfccs,\n",
    "            sync(chroma),\n",
    "            sync(spectral_contrast),\n",
    "            sync(tonnetz),\n",
    "            sync(rms)\n",
    "        ])\n",
    "\n",
    "        return features.T  # shape: (time_steps, n_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# === APPLY FEATURE EXTRACTION ===\n",
    "def extract_features_to_df(df):\n",
    "    tqdm.pandas()\n",
    "    df['features'] = df['path'].progress_apply(extract_advanced_features)\n",
    "    df.dropna(subset=['features'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# === PAD SEQUENCES AND ENCODE LABELS ===\n",
    "def preprocess_data(train_df, val_df, test_df):\n",
    "    # Extract features\n",
    "    train_df = extract_features_to_df(train_df)\n",
    "    val_df = extract_features_to_df(val_df)\n",
    "    test_df = extract_features_to_df(test_df)\n",
    "\n",
    "    # Determine fixed length\n",
    "    fixed_length = int(train_df['features'].apply(len).quantile(0.95))\n",
    "    print(f\"Using fixed sequence length of: {fixed_length}\")\n",
    "\n",
    "    # Pad sequences\n",
    "    X_train = pad_sequences(train_df['features'].tolist(), maxlen=fixed_length, padding='post', truncating='post', dtype='float32')\n",
    "    X_val = pad_sequences(val_df['features'].tolist(), maxlen=fixed_length, padding='post', truncating='post', dtype='float32')\n",
    "    X_test = pad_sequences(test_df['features'].tolist(), maxlen=fixed_length, padding='post', truncating='post', dtype='float32')\n",
    "\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(train_df['emotions'])\n",
    "    y_val = le.transform(val_df['emotions'])\n",
    "    y_test = le.transform(test_df['emotions'])\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    ns, ts, nf = X_train.shape\n",
    "    X_train_scaled = scaler.fit_transform(X_train.reshape(ns * ts, nf)).reshape(ns, ts, nf)\n",
    "    X_val_scaled = scaler.transform(X_val.reshape(-1, nf)).reshape(X_val.shape)\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, nf)).reshape(X_test.shape)\n",
    "\n",
    "    return X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test, scaler, le\n",
    "\n",
    "# === SAVE ARTIFACTS ===\n",
    "def save_processed_data(X_train, y_train, X_val, y_val, X_test, y_test, scaler, le, output_dir='processed_data_advanced'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    np.savez_compressed(\n",
    "        os.path.join(output_dir, 'audio_data_advanced.npz'),\n",
    "        X_train=X_train, X_val=X_val, X_test=X_test,\n",
    "        y_train=y_train, y_val=y_val, y_test=y_test\n",
    "    )\n",
    "    joblib.dump(scaler, os.path.join(output_dir, 'scaler_advanced.joblib'))\n",
    "    joblib.dump(le, os.path.join(output_dir, 'label_encoder_advanced.joblib'))\n",
    "    print(\"\\n--- ALL DONE! ---\")\n",
    "    print(f\"Saved processed files to '{output_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06434750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c33e527ed74506a4c38d9a73f5a3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1012\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8bb2f3ce9543e28e2a2c770768a4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f026921492441a9cb364fdc1842301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fixed sequence length of: 223\n",
      "\n",
      "--- ALL DONE! ---\n",
      "Saved processed files to 'processed_data_advanced'\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, scaler, le = preprocess_data(train, val, test)\n",
    "save_processed_data(X_train, y_train, X_val, y_val, X_test, y_test, scaler, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e84516",
   "metadata": {},
   "source": [
    "# MEL spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e9fad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Mel Spectrograms for training set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0119c072984ab8b2848b365f2100a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Mel Spectrograms for validation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e322e563da6b43c98005dfd2766e86e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DONE! Spectrogram data saved to 'processed_data_spectrogram' ---\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration for Mel Spectrograms ---\n",
    "SAMPLE_RATE = 22050\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "N_MELS = 128  # Height of the spectrogram 'image'\n",
    "FIXED_TIME_STEPS = 174 # Width of the 'image' (~4 seconds)\n",
    "\n",
    "def extract_mel_spectrogram(file_path):\n",
    "    \"\"\" Extracts and resizes a Mel Spectrogram. \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        current_width = mel_spec_db.shape[1]\n",
    "        if current_width > FIXED_TIME_STEPS:\n",
    "            mel_spec_db = mel_spec_db[:, :FIXED_TIME_STEPS]\n",
    "        else:\n",
    "            pad_width = FIXED_TIME_STEPS - current_width\n",
    "            mel_spec_db = np.pad(mel_spec_db, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "# 3. Extract features\n",
    "tqdm.pandas()\n",
    "print(\"Extracting Mel Spectrograms for training set...\")\n",
    "train['features'] = train['path'].progress_apply(extract_mel_spectrogram)\n",
    "print(\"Extracting Mel Spectrograms for validation set...\")\n",
    "val['features'] = val['path'].progress_apply(extract_mel_spectrogram)\n",
    "# ... do the same for test_df ...\n",
    "train.dropna(subset=['features'], inplace=True)\n",
    "val.dropna(subset=['features'], inplace=True)\n",
    "\n",
    "# 4. Create X arrays (stacking the spectrograms)\n",
    "X_train = np.array(train['features'].tolist())\n",
    "X_val = np.array(val['features'].tolist())\n",
    "\n",
    "# 5. Scale features\n",
    "# We scale each spectrogram (image) by its own mean/std. This is a common image processing step.\n",
    "# For simplicity here, we'll use a global scaler.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "\n",
    "# 6. Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train['emotions'])\n",
    "y_val = le.transform(val['emotions'])\n",
    "\n",
    "# 7. Save everything\n",
    "output_dir = 'processed_data_spectrogram'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "np.savez_compressed(\n",
    "    os.path.join(output_dir, 'audio_data_spectrogram.npz'),\n",
    "    X_train=X_train, X_val=X_val, y_train=y_train, y_val=y_val\n",
    ")\n",
    "joblib.dump(scaler, os.path.join(output_dir, 'scaler_spectrogram.joblib'))\n",
    "joblib.dump(le, os.path.join(output_dir, 'label_encoder_spectrogram.joblib'))\n",
    "print(f\"\\n--- DONE! Spectrogram data saved to '{output_dir}' ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308fa15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d938e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47af2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e4f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dbb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# You can reuse the LuongAttention class from before.\n",
    "class LuongAttention(nn.Module):\n",
    "    # ... (same as before) ...\n",
    "\n",
    "class Spec2D_CNN_LSTM_Attention_Model(nn.Module):\n",
    "    def __init__(self, num_classes, lstm_hidden_size=256, lstm_layers=2):\n",
    "        super(Spec2D_CNN_LSTM_Attention_Model, self).__init__()\n",
    "        \n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        \n",
    "        # --- 2D CNN Feature Extractor ---\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Input shape: (batch_size, 1, n_mels, time_steps) -> (B, 1, 128, 174)\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)), # (B, 32, 64, 87)\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)), # (B, 64, 32, 43)\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))  # (B, 128, 16, 21)\n",
    "        )\n",
    "        \n",
    "        # --- Prepare for LSTM ---\n",
    "        # The output of the CNN is a 2D feature map. We treat the time dimension\n",
    "        # as a sequence and flatten the channel and frequency dimensions together.\n",
    "        # Output features from CNN: 128 channels * 16 frequency bins = 2048\n",
    "        self.lstm_input_size = 128 * 16 \n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_input_size, \n",
    "            hidden_size=self.lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.5,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # --- Attention and Classifier ---\n",
    "        self.attention = LuongAttention()\n",
    "        self.fc = nn.Linear(self.lstm_hidden_size * 4, num_classes) # *4 from concat(hidden, context)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, n_mels, time_steps) -> (B, 128, 174)\n",
    "        \n",
    "        # Add a channel dimension for the 2D CNN\n",
    "        x = x.unsqueeze(1) # -> (B, 1, 128, 174)\n",
    "        \n",
    "        # 1. Pass through CNN\n",
    "        x_cnn = self.cnn(x) # -> (B, 128, 16, 21)\n",
    "        \n",
    "        # 2. Prepare for LSTM: need (batch_size, time_steps, features)\n",
    "        # Permute to bring time dimension forward: (B, 21, 128, 16)\n",
    "        x_cnn = x_cnn.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Flatten the channel and frequency dimensions\n",
    "        batch_size, time_steps, C, H = x_cnn.shape\n",
    "        x_lstm_in = x_cnn.reshape(batch_size, time_steps, C * H) # -> (B, 21, 2048)\n",
    "        \n",
    "        # 3. Pass through LSTM\n",
    "        lstm_outputs, (hidden, cell) = self.lstm(x_lstm_in)\n",
    "        \n",
    "        # 4. Apply Attention\n",
    "        last_hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        context_vector, _ = self.attention(lstm_outputs, last_hidden)\n",
    "        \n",
    "        # 5. Combine and Classify\n",
    "        combined_vector = torch.cat((last_hidden, context_vector), dim=1)\n",
    "        logits = self.fc(self.dropout(combined_vector))\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fb3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your Kaggle notebook after loading the NEW spectrogram arrays\n",
    "\n",
    "# --- Model Setup ---\n",
    "NUM_CLASSES = len(le.classes_) # Using the filtered set\n",
    "\n",
    "# Instantiate the correct 2D CNN model\n",
    "model = Spec2D_CNN_LSTM_Attention_Model(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    lstm_hidden_size=256 # Start with a powerful LSTM\n",
    ").to(device)\n",
    "\n",
    "# --- Optimizer, Loss, etc. ---\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "early_stopper = EarlyStopping(patience=7, verbose=True, path='checkpoint_2dcnn.pt')\n",
    "\n",
    "print(\"--- Starting training with 2D-CNN -> LSTM -> Attention model ---\")\n",
    "# Your training loop remains the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
