# Audio Emotion Recognition using Deep Learning

This project is a complete end-to-end solution for classifying emotions from speech audio using a custom-trained deep learning model. The model is deployed as a user-friendly web application using Streamlit. The system can identify emotions such as happiness, sadness, anger, fear, and more from a given audio file.
# Vedio Demo
https://drive.google.com/file/d/1Y7Z_jQ4oq-UGZb4ZxrwzQBpYrqQ2roGj/view?usp=sharing

## Table of Contents
1.  [Features](#features)
2.  [Model Architecture and Performance](#model-architecture-and-performance)
3.  [Project Structure](#project-structure)
4.  [Setup and Installation](#setup-and-installation)
5.  [Usage](#usage)
6.  [Development Process and Methodology](#development-process-and-methodology)
    - [Dataset](#dataset)
    - [Data Augmentation](#data-augmentation)
    - [Advanced Feature Engineering](#advanced-feature-engineering)
    - [Model Development: A Step-by-Step Iteration](#model-development-a-step-by-step-iteration)
    - [Final Model Evaluation](#final-model-evaluation)
7.  [Challenges and Future Work](#challenges-and-future-work)

## Features
- **Real-time Emotion Prediction:** Upload an audio file (`.wav`, `.mp3`, `.flac`) and get an instant emotion classification.
- **Confidence Score:** View the model's confidence in its prediction.
- **Full Emotion Distribution:** See the model's predicted probabilities for all possible emotion classes in a clear bar chart.
- **Interactive Web Interface:** Built with Streamlit for an easy-to-use and responsive user experience.
- **Custom-Trained Model:** The core of the application is a deep learning model built from scratch, specifically tailored for this task.

## Model Architecture and Performance

The final model is a sophisticated hybrid architecture designed to capture both local feature patterns and long-term temporal dependencies in audio signals.

- **Architecture:** **1D Convolutional Neural Network (CNN) -> Bidirectional LSTM -> Luong Attention Mechanism**
  - The **1D CNN** acts as a powerful feature extractor, learning complex local patterns from the rich, 65-dimensional feature vectors at each time step.
  - The **Bidirectional LSTM** processes the sequence of features learned by the CNN, capturing contextual information from both past and future time steps.
  - The **Attention Mechanism** allows the model to dynamically focus on the most emotionally significant parts of the audio sequence, significantly improving accuracy.

- **Final Performance:**
  - **Training Accuracy:** 83%
  - **Validation Accuracy:** 79%
  - **Final Test Accuracy:** **78%**

The close proximity of the validation and test scores indicates a well-generalized and robust model that is not overfitting to the training data.

## Development Process and Methodology

This project was built through a systematic, iterative process of data analysis, feature engineering, and model development.

#### Dataset
The model was trained on the **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)** dataset. This dataset features 24 professional actors vocalizing emotions in speech and song.

#### Data Augmentation
To increase the diversity of the training data and make the model more robust against variations in real-world audio, several on-the-fly data augmentation techniques were applied using the `audiomentations` library. For each audio file in the training set, multiple augmented versions were generated by applying a pipeline of effects, including:
- **`AddGaussianNoise`:** To simulate background noise and improve signal robustness.
- **`TimeStretch`:** To simulate different speaking rates (e.g., fast anger vs. slow sadness).
- **`PitchShift`:** To simulate different vocal tones and pitches.
- **`Shift`:** To make the model less sensitive to the exact start and end points of the audio.

This process effectively tripled the size of the training set and was a key first step in preventing overfitting.

#### Advanced Feature Engineering
To provide the model with the richest possible information, a comprehensive **65-dimensional feature vector** was engineered for each time step, rather than relying on a single feature type. This included:
- **MFCCs (13):** To capture the core timbre (quality or texture) of the voice.
- **Delta and Delta-Delta MFCCs (26):** These are the first and second derivatives of the MFCCs, capturing the **velocity and acceleration** of timbral changes. This is crucial for understanding the dynamics of emotional speech (e.g., a rapid change for surprise vs. a slow change for sadness).
- **Chroma Features (12):** To capture pitch and intonation contours.
- **Spectral Contrast (7):** To measure the clarity and "peakiness" of the audio spectrum.
- **Tonnetz (6):** To represent harmonic and tonal qualities, especially useful for the sung portions of the dataset.
- **RMS Energy (1):** To capture the loudness (amplitude) of the signal.

Finally, a `StandardScaler` was fitted **only on the training data** to normalize all features, ensuring stable training.

#### Model Development: A Step-by-Step Iteration

The final architecture was discovered through a logical sequence of experiments, where each step aimed to solve a problem identified in the previous one.

**1. Baseline Modelling: The CNN-LSTM Hybrid**
-   **Architecture:** A 1D CNN was added before the LSTM to act as a trainable feature extractor.
-   **Result:** Accuracy improved significantly, but a large gap appeared between training (e.g., 84%) and validation (e.g., 70%) accuracy. The problem shifted from underfitting to **severe overfitting**.

**2. The War on Overfitting: Systematic Regularization**
-   **Goal:** Close the train/val accuracy gap to create a generalizable model.
-   **Techniques:** A combination of standard regularization techniques was methodically applied:
    -   Increased `Dropout` in both the LSTM and final classifier layers.
    -   Added `Weight Decay` (L2 Regularization) to the Adam optimizer.
    -   Fine-tuned model complexity (e.g., number of LSTM hidden units).
    -   Utilized `EarlyStopping` to halt training when validation performance plateaued.
-   **Result:** This successfully closed the overfitting gap (e.g., 70% train, 67% val), but at the cost of overall performance. The model was now generalizing well but was too constrained to learn effectivelyâ€”a new state of underfitting.

**3. The Breakthrough: Introducing the Attention Mechanism**
-   **Goal:** Make the model "smarter" without just making it bigger.
-   **Architecture:** A Luong Attention mechanism was added after the LSTM.
-   **Result:** This was the key breakthrough. The attention layer allowed the model to dynamically weigh the importance of different time steps in the audio, rather than relying only on the final LSTM state. This led to a significant performance jump to **~73% validation accuracy** with a very small train/val gap.

**4. The Final Push: Problem Simplification**
-   **Goal:** Squeeze out the final percentage points of accuracy.
-   **Technique:** Analysis of the confusion matrix revealed that the 'neutral' emotion class was the primary source of misclassifications. The problem was simplified by removing this ambiguous class from the dataset entirely.
-   **Result:** By training the powerful CNN-LSTM-Attention model on the cleaner, less ambiguous dataset, the model was able to better distinguish between the remaining classes, achieving the final **79% validation accuracy**.

#### Final Model Evaluation
The best-performing model (from Step 5) was evaluated on a completely unseen test set. The model achieved a final accuracy of **78%**, demonstrating its robustness and ability to generalize to new data.

## Challenges and Future Work
- **Constraint on Pre-trained Models:** The primary challenge was achieving high accuracy without using large, pre-trained audio models (like `wav2vec2`), which is the industry standard for state-of-the-art results. Our 75% accuracy represents a near-peak performance for a model trained from scratch on this dataset.
- **Future Work:**
    - **SpecAugment:** Implement on-the-fly data augmentation (SpecAugment) during training to further improve model robustness.
    - **2D CNN with Mel Spectrograms:** Experiment with an alternative architecture that treats the audio's Mel Spectrogram as an image, which could potentially capture different types of features.
    - **Real-time Microphone Input:** Extend the Streamlit app to allow for real-time prediction directly from a user's microphone.
